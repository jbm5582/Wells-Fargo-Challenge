{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Wells Fargo Challenge (Cleaning).ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "saJS5P1XSjYF"
      },
      "source": [
        "# Data Cleaning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "45qWCENUF1vm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3b50b6ac-d6a3-4d47-9ccb-53a7213876f6"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt \n",
        "!pip install geopandas\n",
        "import geopandas as gpd"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting geopandas\n",
            "  Downloading geopandas-0.10.1-py2.py3-none-any.whl (1.0 MB)\n",
            "\u001b[?25l\r\u001b[K     |▎                               | 10 kB 26.1 MB/s eta 0:00:01\r\u001b[K     |▋                               | 20 kB 26.3 MB/s eta 0:00:01\r\u001b[K     |█                               | 30 kB 18.2 MB/s eta 0:00:01\r\u001b[K     |█▎                              | 40 kB 15.8 MB/s eta 0:00:01\r\u001b[K     |█▋                              | 51 kB 9.4 MB/s eta 0:00:01\r\u001b[K     |██                              | 61 kB 8.6 MB/s eta 0:00:01\r\u001b[K     |██▎                             | 71 kB 8.9 MB/s eta 0:00:01\r\u001b[K     |██▌                             | 81 kB 9.9 MB/s eta 0:00:01\r\u001b[K     |██▉                             | 92 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |███▏                            | 102 kB 8.6 MB/s eta 0:00:01\r\u001b[K     |███▌                            | 112 kB 8.6 MB/s eta 0:00:01\r\u001b[K     |███▉                            | 122 kB 8.6 MB/s eta 0:00:01\r\u001b[K     |████▏                           | 133 kB 8.6 MB/s eta 0:00:01\r\u001b[K     |████▌                           | 143 kB 8.6 MB/s eta 0:00:01\r\u001b[K     |████▊                           | 153 kB 8.6 MB/s eta 0:00:01\r\u001b[K     |█████                           | 163 kB 8.6 MB/s eta 0:00:01\r\u001b[K     |█████▍                          | 174 kB 8.6 MB/s eta 0:00:01\r\u001b[K     |█████▊                          | 184 kB 8.6 MB/s eta 0:00:01\r\u001b[K     |██████                          | 194 kB 8.6 MB/s eta 0:00:01\r\u001b[K     |██████▍                         | 204 kB 8.6 MB/s eta 0:00:01\r\u001b[K     |██████▊                         | 215 kB 8.6 MB/s eta 0:00:01\r\u001b[K     |███████                         | 225 kB 8.6 MB/s eta 0:00:01\r\u001b[K     |███████▎                        | 235 kB 8.6 MB/s eta 0:00:01\r\u001b[K     |███████▋                        | 245 kB 8.6 MB/s eta 0:00:01\r\u001b[K     |████████                        | 256 kB 8.6 MB/s eta 0:00:01\r\u001b[K     |████████▎                       | 266 kB 8.6 MB/s eta 0:00:01\r\u001b[K     |████████▋                       | 276 kB 8.6 MB/s eta 0:00:01\r\u001b[K     |█████████                       | 286 kB 8.6 MB/s eta 0:00:01\r\u001b[K     |█████████▏                      | 296 kB 8.6 MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 307 kB 8.6 MB/s eta 0:00:01\r\u001b[K     |█████████▉                      | 317 kB 8.6 MB/s eta 0:00:01\r\u001b[K     |██████████▏                     | 327 kB 8.6 MB/s eta 0:00:01\r\u001b[K     |██████████▌                     | 337 kB 8.6 MB/s eta 0:00:01\r\u001b[K     |██████████▉                     | 348 kB 8.6 MB/s eta 0:00:01\r\u001b[K     |███████████▏                    | 358 kB 8.6 MB/s eta 0:00:01\r\u001b[K     |███████████▌                    | 368 kB 8.6 MB/s eta 0:00:01\r\u001b[K     |███████████▊                    | 378 kB 8.6 MB/s eta 0:00:01\r\u001b[K     |████████████                    | 389 kB 8.6 MB/s eta 0:00:01\r\u001b[K     |████████████▍                   | 399 kB 8.6 MB/s eta 0:00:01\r\u001b[K     |████████████▊                   | 409 kB 8.6 MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 419 kB 8.6 MB/s eta 0:00:01\r\u001b[K     |█████████████▍                  | 430 kB 8.6 MB/s eta 0:00:01\r\u001b[K     |█████████████▊                  | 440 kB 8.6 MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 450 kB 8.6 MB/s eta 0:00:01\r\u001b[K     |██████████████▎                 | 460 kB 8.6 MB/s eta 0:00:01\r\u001b[K     |██████████████▋                 | 471 kB 8.6 MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 481 kB 8.6 MB/s eta 0:00:01\r\u001b[K     |███████████████▎                | 491 kB 8.6 MB/s eta 0:00:01\r\u001b[K     |███████████████▋                | 501 kB 8.6 MB/s eta 0:00:01\r\u001b[K     |████████████████                | 512 kB 8.6 MB/s eta 0:00:01\r\u001b[K     |████████████████▏               | 522 kB 8.6 MB/s eta 0:00:01\r\u001b[K     |████████████████▌               | 532 kB 8.6 MB/s eta 0:00:01\r\u001b[K     |████████████████▉               | 542 kB 8.6 MB/s eta 0:00:01\r\u001b[K     |█████████████████▏              | 552 kB 8.6 MB/s eta 0:00:01\r\u001b[K     |█████████████████▌              | 563 kB 8.6 MB/s eta 0:00:01\r\u001b[K     |█████████████████▉              | 573 kB 8.6 MB/s eta 0:00:01\r\u001b[K     |██████████████████▏             | 583 kB 8.6 MB/s eta 0:00:01\r\u001b[K     |██████████████████▍             | 593 kB 8.6 MB/s eta 0:00:01\r\u001b[K     |██████████████████▊             | 604 kB 8.6 MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 614 kB 8.6 MB/s eta 0:00:01\r\u001b[K     |███████████████████▍            | 624 kB 8.6 MB/s eta 0:00:01\r\u001b[K     |███████████████████▊            | 634 kB 8.6 MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 645 kB 8.6 MB/s eta 0:00:01\r\u001b[K     |████████████████████▍           | 655 kB 8.6 MB/s eta 0:00:01\r\u001b[K     |████████████████████▊           | 665 kB 8.6 MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 675 kB 8.6 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▎          | 686 kB 8.6 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▋          | 696 kB 8.6 MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 706 kB 8.6 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▎         | 716 kB 8.6 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▋         | 727 kB 8.6 MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 737 kB 8.6 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▏        | 747 kB 8.6 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▌        | 757 kB 8.6 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▉        | 768 kB 8.6 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▏       | 778 kB 8.6 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▌       | 788 kB 8.6 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▉       | 798 kB 8.6 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▏      | 808 kB 8.6 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▍      | 819 kB 8.6 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▊      | 829 kB 8.6 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 839 kB 8.6 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▍     | 849 kB 8.6 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▊     | 860 kB 8.6 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 870 kB 8.6 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▍    | 880 kB 8.6 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▋    | 890 kB 8.6 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 901 kB 8.6 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▎   | 911 kB 8.6 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 921 kB 8.6 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 931 kB 8.6 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▎  | 942 kB 8.6 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▋  | 952 kB 8.6 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▉  | 962 kB 8.6 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▏ | 972 kB 8.6 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▌ | 983 kB 8.6 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▉ | 993 kB 8.6 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▏| 1.0 MB 8.6 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▌| 1.0 MB 8.6 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▉| 1.0 MB 8.6 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 1.0 MB 8.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: shapely>=1.6 in /usr/local/lib/python3.7/dist-packages (from geopandas) (1.7.1)\n",
            "Requirement already satisfied: pandas>=0.25.0 in /usr/local/lib/python3.7/dist-packages (from geopandas) (1.1.5)\n",
            "Collecting fiona>=1.8\n",
            "  Downloading Fiona-1.8.20-cp37-cp37m-manylinux1_x86_64.whl (15.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 15.4 MB 84 kB/s \n",
            "\u001b[?25hCollecting pyproj>=2.2.0\n",
            "  Downloading pyproj-3.2.1-cp37-cp37m-manylinux2010_x86_64.whl (6.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.3 MB 2.4 MB/s \n",
            "\u001b[?25hCollecting munch\n",
            "  Downloading munch-2.5.0-py2.py3-none-any.whl (10 kB)\n",
            "Requirement already satisfied: attrs>=17 in /usr/local/lib/python3.7/dist-packages (from fiona>=1.8->geopandas) (21.2.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from fiona>=1.8->geopandas) (57.4.0)\n",
            "Collecting click-plugins>=1.0\n",
            "  Downloading click_plugins-1.1.1-py2.py3-none-any.whl (7.5 kB)\n",
            "Requirement already satisfied: click>=4.0 in /usr/local/lib/python3.7/dist-packages (from fiona>=1.8->geopandas) (7.1.2)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.7/dist-packages (from fiona>=1.8->geopandas) (2021.5.30)\n",
            "Requirement already satisfied: six>=1.7 in /usr/local/lib/python3.7/dist-packages (from fiona>=1.8->geopandas) (1.15.0)\n",
            "Collecting cligj>=0.5\n",
            "  Downloading cligj-0.7.2-py3-none-any.whl (7.1 kB)\n",
            "Requirement already satisfied: numpy>=1.15.4 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.25.0->geopandas) (1.19.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.25.0->geopandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.25.0->geopandas) (2018.9)\n",
            "Installing collected packages: munch, cligj, click-plugins, pyproj, fiona, geopandas\n",
            "Successfully installed click-plugins-1.1.1 cligj-0.7.2 fiona-1.8.20 geopandas-0.10.1 munch-2.5.0 pyproj-3.2.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fk-43B8ILT8f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "11de92b8-545c-471a-e2ac-6ae7e02fd326"
      },
      "source": [
        "# Reading in all data for cleaning and preprocessing #\n",
        "\n",
        "test_data = pd.read_excel(\"/content/drive/MyDrive/Wells Fargo Project/b765dc3d8076-testset_for_participants.xlsx\")\n",
        "train_data = pd.read_excel(\"/content/drive/MyDrive/Wells Fargo Project/b765dc3d8076-trainset+(1).xlsx\")\n",
        "census_data = pd.read_csv(\"/content/drive/MyDrive/Wells Fargo Project/ACSST5Y2019.S1901_data_with_overlays_2021-09-09T142232.csv\")\n",
        "df = gpd.read_file('/content/drive/MyDrive/Wells Fargo Project/tl_2020_us_zcta520/tl_2020_us_zcta520.shp')\n",
        "fraud_rates = pd.read_csv(\"/content/drive/MyDrive/Wells Fargo Project/Fraud.txt\",sep = '\\t', header = None).rename(columns = {0:'overall_rank',1:'state',2:'fraud_score',\n",
        "                                                                                                                              3:'id_theft_rank', 4:'fraud_rank',5:'policy rank'})"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py:2718: DtypeWarning: Columns (2,3,34,35,66,67,98,99) have mixed types.Specify dtype option on import or set low_memory=False.\n",
            "  interactivity=interactivity, compiler=compiler, result=result)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w1xTY8sPD9hn"
      },
      "source": [
        "# Cleaning ACS 5-year 2019 data, selecting the desired rows from the dataset and merging them to the testing and training dataset by zipcode #\n",
        "\n",
        "zip_income = census_data[['NAME','S1901_C01_012E','S1901_C01_013E']][1:].rename(columns = {'NAME':'zipcode','S1901_C01_012E':'median_income','S1901_C01_013E':'mean_income'})\n",
        "#zip_cleaning = zip_income.copy()\n",
        "columns = list(zip_income.columns)\n",
        "series_list = [zip_income.median_income,zip_income.mean_income]\n",
        "cleaned_dict = {}\n",
        "mender = []\n",
        "i = 0\n",
        "\n",
        "# Extracting zipcode values from series object in zip_income dataframe and appending to an empty dictionary #\n",
        "\n",
        "zip_income['zipcode'] = zip_income.zipcode.str.extract(r'\\w+ (\\d+)')\n",
        "cleaned_dict['zipcode'] = list(zip_income.zipcode)\n",
        "\n",
        "# Handling string value objects in dataset with mixed (character and numerical string values) the goal is to fill character strings with 0 and turn the numerical strings into integers\n",
        "\n",
        "for series in series_list: # isolates a single series object in list of series\n",
        "  i += 1\n",
        "  for element in range(1,len(series) + 1): # isolates every element within the selected series\n",
        "    try: # Will conduct the code in the next two lines below until a ValueError occurs\n",
        "      make_int = int(series[element])\n",
        "      mender.append(make_int)\n",
        "    except ValueError:\n",
        "      series[element] = np.nan\n",
        "      mender.append(series[element])\n",
        "  cleaned_dict[columns[i]] = mender # Names dictionary key based on iteratively selected column name uses mender list as value to form a new key:value pair in dictionary\n",
        "  mender = [] # Clears list for use in iteration of for loop to the next list\n",
        "\n",
        "zip_income = pd.DataFrame(cleaned_dict).fillna(0)\n",
        "zip_income = zip_income.astype({'zipcode':'int64'})"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QSafWMjuR6Dv"
      },
      "source": [
        "zip_income.to_csv(\"/content/drive/MyDrive/Wells Fargo Project/zip_income.csv\")"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UJVM8qMmVUwZ"
      },
      "source": [
        "# Handling NaNs in dataframes based on column dtype replacing them with either a zero or missing #\n",
        "\n",
        "series_dict = {}\n",
        "\n",
        "for column in train_data.columns:\n",
        "  Series = train_data[column]\n",
        "  get_dtype = Series.dtype\n",
        "  get_dtype = '{get_dtype}'.format(get_dtype = get_dtype)\n",
        "  if (get_dtype == 'float64'):\n",
        "    Series = Series.fillna(0.0)\n",
        "    series_dict['{column}'.format(column = column)] = list(Series)\n",
        "  elif (get_dtype == 'int64'):\n",
        "    Series = Series.fillna(0)\n",
        "    series_dict['{column}'.format(column = column)] = list(Series)  \n",
        "  elif (get_dtype == 'object'):\n",
        "    Series = Series.fillna('missing') \n",
        "    series_dict['{column}'.format(column = column)] = list(Series)\n",
        "  else:\n",
        "    series_dict['{column}'.format(column = column)] = list(Series)\n",
        "\n",
        "train_data = pd.DataFrame(series_dict)\n",
        "\n",
        "# Repeating above code for test dataset #\n",
        "\n",
        "series_dict1 = {}\n",
        "\n",
        "for column in test_data.columns:\n",
        "  Series = test_data[column]\n",
        "  get_dtype = Series.dtype\n",
        "  get_dtype = '{get_dtype}'.format(get_dtype = get_dtype)\n",
        "  if (get_dtype == 'float64'):\n",
        "    Series = Series.fillna(0.0)\n",
        "    series_dict1['{column}'.format(column = column)] = list(Series)\n",
        "  elif (get_dtype == 'int64'):\n",
        "    Series = Series.fillna(0)\n",
        "    series_dict1['{column}'.format(column = column)] = list(Series)  \n",
        "  elif (get_dtype == 'object'):\n",
        "    Series = Series.fillna('missing') \n",
        "    series_dict1['{column}'.format(column = column)] = list(Series)\n",
        "  else:\n",
        "    series_dict1['{column}'.format(column = column)] = list(Series)\n",
        "\n",
        "test_data = pd.DataFrame(series_dict1)\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NKiv3NjNd2Ux"
      },
      "source": [
        "# identifies all zipcodes in the ACS 5-year 2019 dataset which are also in the training dataset it reorganizes the dataframe and a merge step is performed to combine the two datasets\n",
        "\n",
        "all_zips = list(zip_income.zipcode)\n",
        "zip_income_cleaned = pd.DataFrame()\n",
        "\n",
        "for zip in all_zips:\n",
        "  if zip in list(train_data.CUST_ZIP):\n",
        "    in_zip = zip_income[zip_income.zipcode == zip]\n",
        "    zip_income_cleaned = pd.concat([zip_income_cleaned,in_zip], axis = 0)\n",
        "\n",
        "\n",
        "train_data = train_data.merge(zip_income_cleaned, left_on = 'CUST_ZIP', right_on = 'zipcode')\n",
        "zip_income_cleaned = pd.DataFrame()\n",
        "\n",
        "# Repeat of the previous steps for the testing data\n",
        "\n",
        "for zip in all_zips:\n",
        "  if zip in list(test_data.CUST_ZIP):\n",
        "    in_zip = zip_income[zip_income.zipcode == zip]\n",
        "    zip_income_cleaned = pd.concat([zip_income_cleaned,in_zip], axis = 0)\n",
        "\n",
        "test_data = test_data.merge(zip_income_cleaned, left_on = 'CUST_ZIP', right_on = 'zipcode')"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iVjzicvXz_9V"
      },
      "source": [
        "# identifies states present in fraud_rates dataset also present in train and test datasets and merges based on the results of the initial state matching #\n",
        "\n",
        "fraud_rates_cleaned = pd.DataFrame()\n",
        "\n",
        "for states in list(fraud_rates.state):\n",
        "  if states in list(train_data.CUST_STATE):\n",
        "    in_state = fraud_rates[fraud_rates.state == states]\n",
        "    fraud_rates_cleaned = pd.concat([fraud_rates_cleaned, in_state], axis = 0)\n",
        "\n",
        "train_data = train_data.merge(fraud_rates_cleaned, left_on = 'CUST_STATE', right_on = 'state')\n",
        "\n",
        "fraud_rates_cleaned = pd.DataFrame()\n",
        "\n",
        "# Repeating steps from code above #\n",
        "\n",
        "for states in list(fraud_rates.state):\n",
        "  if states in list(test_data.CUST_STATE):\n",
        "    in_state = fraud_rates[fraud_rates.state == states]\n",
        "    fraud_rates_cleaned = pd.concat([fraud_rates_cleaned, in_state], axis = 0)\n",
        "\n",
        "test_data = test_data.merge(fraud_rates_cleaned, left_on = 'CUST_STATE', right_on = 'state')\n",
        "\n",
        "\n"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v3xAs9hFaqCc"
      },
      "source": [
        "# Dropping rows from both the training and testing datasets #\n",
        "\n",
        "train_data = train_data.drop(columns = ['ACTN_CD','ACTN_INTNL_TXT','TRAN_TYPE_CD','ACTVY_DT','zipcode'])\n",
        "test_data = test_data.drop(columns = ['ACTN_CD','ACTN_INTNL_TXT','TRAN_TYPE_CD','ACTVY_DT','zipcode'])"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pmWfWtnbqytH"
      },
      "source": [
        "# Categorizing CUST_AGE column #\n",
        "\n",
        "def age_categorize(dataset):\n",
        "\n",
        "  age_cats = []\n",
        "  for age in list(dataset.CUST_AGE.astype('int64')):\n",
        "    if (age >= 0) & (age <= 29):\n",
        "      age_cats.append('Young Adult')\n",
        "    elif (age >= 30) & (age <= 59):\n",
        "      age_cats.append('Adult')\n",
        "    elif (age >= 60):\n",
        "      age_cats.append('Senior')\n",
        "  \n",
        "  dataset['CUST_AGE'] = age_cats\n",
        "\n",
        "  return dataset\n",
        "\n",
        "def get_times(dataset,column, year_since = 0):\n",
        "\n",
        "    dataset['year'] = dataset[column].str.extract(r'\\d{,100}/\\d{,100}/(\\d{,100}) \\d{1,2}:\\d{1,2}:\\d{1,2}').fillna(9999)\n",
        "    t_list = []\n",
        "    int_list = []\n",
        "\n",
        "    if year_since == 0:\n",
        "\n",
        "        dataset['month'] = dataset[column].str.extract(r'(\\d{,100})/\\d{,100}/\\d{,100} \\d{1,2}:\\d{1,2}:\\d{1,2}')\n",
        "        dataset['tod'] = dataset[column].str.extract(r'\\d{,100}/\\d{,100}/\\d{,100} (\\d{1,2}):\\d{1,2}:\\d{1,2}')\n",
        "\n",
        "        for value in range(0,len(dataset.tod)):   \n",
        "            try:\n",
        "                make_int = int(dataset.tod[value])\n",
        "                int_list.append(make_int)\n",
        "            except ValueError:\n",
        "                int_list.append(np.nan)\n",
        "\n",
        "        for hour in int_list:\n",
        "            if (hour >= 0) & (hour <= 5):\n",
        "                t_list.append('Early Morning')\n",
        "            elif (hour >=6) & (hour <= 11):\n",
        "                t_list.append('Morning')\n",
        "            elif (hour >= 12) & (hour <= 17):\n",
        "                t_list.append('Afternoon')\n",
        "            elif (hour >= 18) & (hour <= 23):\n",
        "                t_list.append('Evening')\n",
        "            else:\n",
        "                t_list.append('Unknown')\n",
        "        \n",
        "\n",
        "        dataset[column] = t_list\n",
        "        dataset = dataset.drop(columns = ['tod'])\n",
        "\n",
        "\n",
        "    elif year_since == 1:\n",
        "\n",
        "        dataset['year'] = dataset.year.astype('int64')\n",
        "        dataset['years_since'] = 2021 - dataset['year']\n",
        "        dataset = dataset.drop(columns = ['year'])\n",
        "\n",
        "        for years in list(dataset['years_since']):\n",
        "            if (years >= 0) & (years <=1):\n",
        "                t_list.append('1 year or Less')\n",
        "            elif (years > 1) & (years <= 3):\n",
        "                t_list.append('2-3 years')\n",
        "            elif (years > 3) & (years <= 4):\n",
        "                t_list.append('3-4 years')\n",
        "            elif (years >= 5):\n",
        "                t_list.append('5+ years')\n",
        "            elif (years < 0):\n",
        "                t_list.append('Unknown')\n",
        "    \n",
        "        dataset[column] = t_list\n",
        "        dataset = dataset.drop(columns = ['years_since'])\n",
        "\n",
        "    return dataset\n",
        "\n",
        "\n",
        "\n",
        "    \n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i6G4TMlXg_8E"
      },
      "source": [
        "# Modifying train_data\n",
        "train_data['AGE'] = train_data.CUST_AGE\n",
        "train_data = age_categorize(train_data)\n",
        "train_data = get_times(train_data,'PWD_UPDT_TS',year_since = 1)\n",
        "train_data = get_times(train_data,'PH_NUM_UPDT_TS',year_since = 1)\n",
        "train_data = get_times(train_data,'TRAN_TS')"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zb-1sVJYP0J3"
      },
      "source": [
        "test_data['AGE'] = test_data.CUST_AGE\n",
        "test_data = age_categorize(test_data)\n",
        "test_data = get_times(test_data,'PWD_UPDT_TS',year_since = 1)\n",
        "test_data = get_times(test_data,'PH_NUM_UPDT_TS',year_since = 1)\n",
        "test_data = get_times(test_data,'TRAN_TS')"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ntIseGHWOtsJ"
      },
      "source": [
        "train_data1 = train_data[train_data.FRAUD_NONFRAUD == 'Fraud']\n",
        "fraud_hits = train_data1.groupby(['CUST_ZIP'])[['CUST_ZIP']].count().rename({'CUST_ZIP':'fraud_hits'}, axis = 'columns').reset_index().astype({'CUST_ZIP':'int64'})"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f2VLuaipO_DR"
      },
      "source": [
        "med_income = list()\n",
        "zip_list = list()\n",
        "\n",
        "for zip in list(df.ZCTA5CE20.astype('int64')):\n",
        "  zip_list.append(zip)\n",
        "  if zip in list(train_data.CUST_ZIP):\n",
        "    get_income = list(train_data[train_data.CUST_ZIP == zip]['median_income'])\n",
        "    med_income.append(get_income[0])\n",
        "  elif zip not in list(train_data.CUST_ZIP):\n",
        "    med_income.append(0)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1PGwHY9mPAkk"
      },
      "source": [
        "f_hits = list()\n",
        "zip_list = list()\n",
        "\n",
        "for zip in list(df.ZCTA5CE20):\n",
        "  zip_list.append(zip)\n",
        "  zip = int(zip)\n",
        "  if zip in list(fraud_hits.CUST_ZIP):\n",
        "    get_hits = list(fraud_hits[fraud_hits.CUST_ZIP == zip].fraud_hits)[0]\n",
        "    f_hits.append(get_hits)\n",
        "  elif zip not in list(fraud_hits.CUST_ZIP):\n",
        "    f_hits.append(0)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N2M9nHuFPE0H"
      },
      "source": [
        "#zip_income = zip_income.drop(['Unnamed: 0','mean_income'],axis = 1)\n",
        "zip_income = zip_income.astype({'zipcode':'int64'})"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oycL_SvcPN-X"
      },
      "source": [
        "data = {'ZIP':zip_list,'CUST_ZIP_median_income':med_income,'fraud_hits':f_hits}\n",
        "geo_df = df.astype({'ZCTA5CE20':'int64'}).merge(pd.DataFrame(data).astype({'ZIP':'int64'}),left_on = 'ZCTA5CE20',right_on = 'ZIP').drop('ZIP',axis = 1)\n",
        "geo_df = geo_df.merge(zip_income,left_on = 'ZCTA5CE20',right_on = 'zipcode').drop('zipcode',axis = 1)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mnnJeotxP09V"
      },
      "source": [
        "alert_zips = train_data.groupby(['CUST_ZIP'])[['CUST_ZIP']].count().rename({'CUST_ZIP':'alert_counts'}, axis = 'columns').reset_index().astype({'CUST_ZIP':'int64'})\n",
        "alert_ratio1 = fraud_hits.merge(alert_zips,left_on = 'CUST_ZIP', right_on = 'CUST_ZIP').sort_values('alert_counts',ascending = False)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RmzNPQKPQNZH"
      },
      "source": [
        "zip_list = list()\n",
        "f_list = list()\n",
        "for zip in list(train_data.CUST_ZIP):\n",
        "  zip_list.append(zip)\n",
        "  if zip in list(fraud_hits.CUST_ZIP):\n",
        "    get_fraud = list(fraud_hits[fraud_hits.CUST_ZIP == zip]['fraud_hits'])[0]\n",
        "    f_list.append(get_fraud)\n",
        "  elif zip not in list(fraud_hits.CUST_ZIP):\n",
        "    f_list.append(0)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "1D3mNdsDQXcK",
        "outputId": "4bab0454-b3d9-4a9b-c0f7-e7a51527c04e"
      },
      "source": [
        "alert_ratio = pd.DataFrame({'CUST_ZIP':zip_list,'fraud_hits':f_list}).merge(alert_zips,left_on = 'CUST_ZIP',right_on = 'CUST_ZIP')\n",
        "alert_ratio['fraud_alert_ratio'] = alert_ratio.fraud_hits/alert_ratio.alert_counts\n",
        "alert_ratio"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>CUST_ZIP</th>\n",
              "      <th>fraud_hits</th>\n",
              "      <th>alert_counts</th>\n",
              "      <th>fraud_alert_ratio</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>89002</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>0.333333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>89002</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>0.333333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>89002</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>0.333333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>89822</td>\n",
              "      <td>0</td>\n",
              "      <td>5</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>89822</td>\n",
              "      <td>0</td>\n",
              "      <td>5</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13685</th>\n",
              "      <td>25430</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13686</th>\n",
              "      <td>25430</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13687</th>\n",
              "      <td>25430</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13688</th>\n",
              "      <td>26330</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13689</th>\n",
              "      <td>25530</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>13690 rows × 4 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       CUST_ZIP  fraud_hits  alert_counts  fraud_alert_ratio\n",
              "0         89002           1             3           0.333333\n",
              "1         89002           1             3           0.333333\n",
              "2         89002           1             3           0.333333\n",
              "3         89822           0             5           0.000000\n",
              "4         89822           0             5           0.000000\n",
              "...         ...         ...           ...                ...\n",
              "13685     25430           3             3           1.000000\n",
              "13686     25430           3             3           1.000000\n",
              "13687     25430           3             3           1.000000\n",
              "13688     26330           0             1           0.000000\n",
              "13689     25530           0             1           0.000000\n",
              "\n",
              "[13690 rows x 4 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W6q09seoQex8"
      },
      "source": [
        "train_data = pd.concat([train_data,alert_ratio[['fraud_hits','alert_counts','fraud_alert_ratio']]],axis = 1)\n"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K1SEusxWQtkt"
      },
      "source": [
        "zip_list = list()\n",
        "fraud_rate = list()\n",
        "alert = list()\n",
        "f_list = list()\n",
        "for zip in list(test_data.CUST_ZIP):\n",
        "  zip_list.append(zip)\n",
        "  if zip in list(alert_ratio.CUST_ZIP):\n",
        "    get_rate = list(alert_ratio[alert_ratio.CUST_ZIP == zip]['fraud_alert_ratio'])[0]\n",
        "    get_hits = list(alert_ratio[alert_ratio.CUST_ZIP == zip]['fraud_hits'])[0]\n",
        "    get_counts = list(alert_ratio[alert_ratio.CUST_ZIP == zip]['alert_counts'])[0]\n",
        "    fraud_rate.append(get_rate)\n",
        "    alert.append(get_counts)\n",
        "    f_list.append(get_hits)\n",
        "  elif zip not in list(alert_ratio.CUST_ZIP):\n",
        "    fraud_rate.append(0.0) \n",
        "    alert.append(0.0)\n",
        "    f_list.append(0.0)"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jMr5mhFRQy4W"
      },
      "source": [
        "test_data = pd.concat([test_data,pd.Series(f_list).rename('fraud_hits'),pd.Series(alert).rename('alert_counts'),\n",
        "           pd.Series(fraud_rate).rename('fraud_alert_ratio')],axis = 1)\n"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SCNM57JIn3gV"
      },
      "source": [
        " train_data.to_csv(\"/content/drive/MyDrive/Wells Fargo Project/train_data.csv\")"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_I2kk5qfqLaq"
      },
      "source": [
        "test_data.to_csv(\"/content/drive/MyDrive/Wells Fargo Project/test_data.csv\")"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PmGxhicqWmV3",
        "outputId": "a895e616-b524-421d-ffc2-7b130be4744c"
      },
      "source": [
        "geo_df.to_file(\"/content/drive/MyDrive/Wells Fargo Project/geo_df/geo_df.shp\")"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:1: UserWarning: Column names longer than 10 characters will be truncated when saved to ESRI Shapefile.\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wnyrr80TfbTr"
      },
      "source": [
        "alert_ratio1.to_csv(\"/content/drive/MyDrive/Wells Fargo Project/alert_ratio1.csv\")\n",
        "alert_ratio.to_csv(\"/content/drive/MyDrive/Wells Fargo Project/alert_ratio.csv\")\n"
      ],
      "execution_count": 26,
      "outputs": []
    }
  ]
}